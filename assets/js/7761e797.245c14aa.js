"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[138],{5398:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"Isaac/computer-vision","title":"Chapter 4: Computer Vision in Isaac","description":"4.1 Overview of Computer Vision for Robotics","source":"@site/docs/04-Isaac/04-computer-vision.md","sourceDirName":"04-Isaac","slug":"/Isaac/computer-vision","permalink":"/Physical-AI-Book/Isaac/computer-vision","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: NVIDIA Isaac ROS","permalink":"/Physical-AI-Book/Isaac/isaac-ros"},"next":{"title":"Chapter 5: NAV2 and Autonomous Navigation","permalink":"/Physical-AI-Book/Isaac/nav2"}}');var a=n(4848),o=n(8453);const r={},s="Chapter 4: Computer Vision in Isaac",c={},l=[{value:"4.1 Overview of Computer Vision for Robotics",id:"41-overview-of-computer-vision-for-robotics",level:2},{value:"4.1.1 Perception Pipeline",id:"411-perception-pipeline",level:3},{value:"4.1.1.1 Image Preprocessing",id:"4111-image-preprocessing",level:4},{value:"4.1.1.1.1 Camera Calibration",id:"41111-camera-calibration",level:5},{value:"4.1.1.1.1.1 Intrinsic Parameters",id:"411111-intrinsic-parameters",level:6},{value:"4.2 Deep Learning for Vision",id:"42-deep-learning-for-vision",level:2},{value:"4.2.1 Object Detection",id:"421-object-detection",level:3},{value:"4.2.1.1 Instance Segmentation",id:"4211-instance-segmentation",level:4},{value:"4.2.1.1.1 Semantic Segmentation",id:"42111-semantic-segmentation",level:5},{value:"4.2.1.1.1.1 Real-time Performance",id:"421111-real-time-performance",level:6}];function d(e){const i={h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",h6:"h6",header:"header",p:"p",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.header,{children:(0,a.jsx)(i.h1,{id:"chapter-4-computer-vision-in-isaac",children:"Chapter 4: Computer Vision in Isaac"})}),"\n",(0,a.jsx)(i.h2,{id:"41-overview-of-computer-vision-for-robotics",children:"4.1 Overview of Computer Vision for Robotics"}),"\n",(0,a.jsx)(i.p,{children:"Computer vision is a crucial aspect of AI-powered robotics, enabling robots to perceive and understand their environment through visual data. Isaac provides tools and libraries for accelerated CV workloads."}),"\n",(0,a.jsx)(i.h3,{id:"411-perception-pipeline",children:"4.1.1 Perception Pipeline"}),"\n",(0,a.jsx)(i.p,{children:"A typical perception pipeline involves acquiring raw sensor data (e.g., images, point clouds), processing it, extracting features, and interpreting the scene for various robotic tasks."}),"\n",(0,a.jsx)(i.h4,{id:"4111-image-preprocessing",children:"4.1.1.1 Image Preprocessing"}),"\n",(0,a.jsx)(i.p,{children:"Preprocessing steps like noise reduction, color correction, and rectification are often necessary to enhance image quality and prepare data for subsequent computer vision algorithms."}),"\n",(0,a.jsx)(i.h5,{id:"41111-camera-calibration",children:"4.1.1.1.1 Camera Calibration"}),"\n",(0,a.jsx)(i.p,{children:"Accurate camera calibration is essential to correct for lens distortions and to determine the intrinsic and extrinsic parameters of the camera, allowing for precise 3D measurements."}),"\n",(0,a.jsx)(i.h6,{id:"411111-intrinsic-parameters",children:"4.1.1.1.1.1 Intrinsic Parameters"}),"\n",(0,a.jsx)(i.p,{children:"Intrinsic parameters describe the camera's optical properties and how it projects 3D points into a 2D image, including focal length, principal point, and skew coefficient."}),"\n",(0,a.jsx)(i.h2,{id:"42-deep-learning-for-vision",children:"4.2 Deep Learning for Vision"}),"\n",(0,a.jsx)(i.p,{children:"Deep learning, particularly convolutional neural networks (CNNs), has revolutionized computer vision, enabling robots to perform advanced perception tasks like object detection, segmentation, and pose estimation."}),"\n",(0,a.jsx)(i.h3,{id:"421-object-detection",children:"4.2.1 Object Detection"}),"\n",(0,a.jsx)(i.p,{children:"Object detection algorithms (e.g., YOLO, Faster R-CNN) identify and localize objects within an image by drawing bounding boxes around them and classifying their categories."}),"\n",(0,a.jsx)(i.h4,{id:"4211-instance-segmentation",children:"4.2.1.1 Instance Segmentation"}),"\n",(0,a.jsx)(i.p,{children:"Instance segmentation goes beyond bounding boxes to segment each individual object instance in an image at a pixel level, providing more detailed spatial information."}),"\n",(0,a.jsx)(i.h5,{id:"42111-semantic-segmentation",children:"4.2.1.1.1 Semantic Segmentation"}),"\n",(0,a.jsx)(i.p,{children:"Semantic segmentation classifies each pixel in an image into a predefined category, providing a dense understanding of the scene's content without differentiating individual instances."}),"\n",(0,a.jsx)(i.h6,{id:"421111-real-time-performance",children:"4.2.1.1.1.1 Real-time Performance"}),"\n",(0,a.jsx)(i.p,{children:"For robotic applications, deep learning vision models must often operate in real-time, requiring efficient architectures and hardware acceleration provided by platforms like Isaac."})]})}function p(e={}){const{wrapper:i}={...(0,o.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>r,x:()=>s});var t=n(6540);const a={},o=t.createContext(a);function r(e){const i=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function s(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(o.Provider,{value:i},e.children)}}}]);