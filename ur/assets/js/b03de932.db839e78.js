"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[281],{3667:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>s,contentTitle:()=>l,default:()=>g,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Vla/cognitive-planning","title":"Chapter 3: Cognitive Planning for VLA","description":"3.1 Introduction to Cognitive Planning","source":"@site/docs/06-Vla/03-cognitive-planning.md","sourceDirName":"06-Vla","slug":"/Vla/cognitive-planning","permalink":"/Physical-AI-Book/ur/Vla/cognitive-planning","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Conversational AI for Robotics","permalink":"/Physical-AI-Book/ur/Vla/conversational-ai"},"next":{"title":"Chapter 4: Multimodal Learning for VLA","permalink":"/Physical-AI-Book/ur/Vla/multimodal"}}');var a=i(4848),o=i(8453);const r={},l="Chapter 3: Cognitive Planning for VLA",s={},c=[{value:"3.1 Introduction to Cognitive Planning",id:"31-introduction-to-cognitive-planning",level:2},{value:"3.1.1 Hierarchical Task Networks (HTN)",id:"311-hierarchical-task-networks-htn",level:3},{value:"3.1.1.1 State-Space Planning",id:"3111-state-space-planning",level:4},{value:"3.1.1.1.1 PDDL (Planning Domain Definition Language)",id:"31111-pddl-planning-domain-definition-language",level:5},{value:"3.2 Learning for Cognitive Planning",id:"32-learning-for-cognitive-planning",level:2},{value:"3.2.1 Reinforcement Learning for Planning",id:"321-reinforcement-learning-for-planning",level:3},{value:"3.2.1.1 Learning Action Models",id:"3211-learning-action-models",level:4}];function d(n){const e={h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",p:"p",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-3-cognitive-planning-for-vla",children:"Chapter 3: Cognitive Planning for VLA"})}),"\n",(0,a.jsx)(e.h2,{id:"31-introduction-to-cognitive-planning",children:"3.1 Introduction to Cognitive Planning"}),"\n",(0,a.jsx)(e.p,{children:"Cognitive planning in VLA systems allows robots to reason about tasks, break them down into sub-goals, and generate action sequences that achieve desired outcomes, often under uncertainty."}),"\n",(0,a.jsx)(e.h3,{id:"311-hierarchical-task-networks-htn",children:"3.1.1 Hierarchical Task Networks (HTN)"}),"\n",(0,a.jsx)(e.p,{children:"HTN planners represent tasks and methods hierarchically, decomposing complex tasks into simpler ones until primitive actions are reached. This mirrors human problem-solving."}),"\n",(0,a.jsx)(e.h4,{id:"3111-state-space-planning",children:"3.1.1.1 State-Space Planning"}),"\n",(0,a.jsx)(e.p,{children:"State-space planners search through possible states of the world to find a sequence of actions that transforms the initial state into a desired goal state."}),"\n",(0,a.jsx)(e.h5,{id:"31111-pddl-planning-domain-definition-language",children:"3.1.1.1.1 PDDL (Planning Domain Definition Language)"}),"\n",(0,a.jsx)(e.p,{children:"PDDL is a standardized language used to define planning problems, including actions, preconditions, and effects, allowing planners to solve various robotic tasks."}),"\n",(0,a.jsx)(e.h2,{id:"32-learning-for-cognitive-planning",children:"3.2 Learning for Cognitive Planning"}),"\n",(0,a.jsx)(e.p,{children:"Robots can learn to plan more effectively through experience, adapting to new environments and improving their ability to solve novel tasks."}),"\n",(0,a.jsx)(e.h3,{id:"321-reinforcement-learning-for-planning",children:"3.2.1 Reinforcement Learning for Planning"}),"\n",(0,a.jsx)(e.p,{children:"Reinforcement learning can be used to train agents to learn optimal policies that guide their planning process, particularly in environments with uncertain outcomes."}),"\n",(0,a.jsx)(e.h4,{id:"3211-learning-action-models",children:"3.2.1.1 Learning Action Models"}),"\n",(0,a.jsx)(e.p,{children:"Robots can learn action models (preconditions and effects of actions) from observation or interaction, reducing the need for explicit programming of every possible action."})]})}function g(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>l});var t=i(6540);const a={},o=t.createContext(a);function r(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:r(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);